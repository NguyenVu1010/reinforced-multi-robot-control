#!/usr/bin/env python3

# trainer_logic/training_orchestrator.py

import rospy
import torch
import numpy as np
from gazebo_msgs.srv import SetModelState
from gazebo_msgs.msg import ModelState
from geometry_msgs.msg import Point
import random
from tf.transformations import quaternion_from_euler
# Gi·∫£ ƒë·ªãnh c√°c l·ªõp n√†y ƒë√£ ƒë∆∞·ª£c import ƒë√∫ng c√°ch t·ª´ c√°c package t∆∞∆°ng ·ª©ng
# S·ª≠a l·∫°i ƒë∆∞·ªùng d·∫´n import cho ƒë√∫ng v·ªõi c·∫•u tr√∫c package c·ªßa b·∫°n
from env.environment import MiRPathFollowingEnv 
from core.memory import Memory
from core.trainer import PPOTrainer
from mir_control.srv import RequestPath, RequestPathRequest
from gazebo_msgs.msg import ModelStates
from utils.training_logger import TrainingLogger
class TrainingOrchestrator:
    """
    ƒêi·ªÅu ph·ªëi to√†n b·ªô v√≤ng l·∫∑p hu·∫•n luy·ªán, t√°i t·∫°o ch√≠nh x√°c logic g·ªëc,
    v·ªõi m·ªôt v√≤ng l·∫∑p tu·∫ßn t·ª± qua nhi·ªÅu instance m√¥i tr∆∞·ªùng.
    """
    def __init__(self, config_manager, model_manager, system_controller):
        self.cfg_manager = config_manager
        self.model_manager = model_manager
        self.sys_controller = system_controller
        
        self.num_robots = self.cfg_manager.env_cfg['num_robots']
        self.robot_names = [f'robot{i+1}' for i in range(self.num_robots)]
        self.last_cycle_total_rewards_per_robot = {name: 0.0 for name in self.robot_names}
        # --- Kh·ªüi t·∫°o NHI·ªÄU instance m√¥i tr∆∞·ªùng ---
        rospy.loginfo(f"Kh·ªüi t·∫°o {self.num_robots} m√¥i tr∆∞·ªùng Gym cho: {self.robot_names}")
        self.envs = {
            name: MiRPathFollowingEnv(name, self.robot_names, self.cfg_manager) 
            for name in self.robot_names
        }
        rospy.wait_for_service('/gazebo/set_model_state')
        self.set_model_state = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)
        # --- Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn kh√°c ---
        ppo_trainer_params = self.cfg_manager.train_cfg.copy()
        ppo_trainer_params.pop('epochs', None)
        ppo_trainer_params.pop('log_interval', None)
        self.logger = TrainingLogger(self.model_manager.save_dir, self.robot_names,self.cfg_manager.checkpoint_cfg['log_filename'])
        self.trainer = PPOTrainer(
            policy_net=self.model_manager.policy_net, 
            value_net=self.model_manager.value_net, 
            **ppo_trainer_params
        )
        self.memory = Memory()
        self.steps_in_task = {name: 0 for name in self.robot_names}
        self.start_cycle = 0
        self.latest_model_states = None

        # C√°c bi·∫øn ƒë·ªÉ logging
        self.last_cycle_avg_reward_per_robot = {name: 0.0 for name in self.robot_names}
        self.last_cycle_total_reward_per_robot = {name: 0.0 for name in self.robot_names}
        self.last_cycle_avg_system_reward = 0.0
        
        # K·∫øt n·ªëi service v√† subscriber
        self._connect_ros_infra()
        self.model_states_sub = rospy.Subscriber(
            '/gazebo/model_states', ModelStates, self._model_states_callback, queue_size=1
        )
        
    def _model_states_callback(self, msg):
        self.latest_model_states = msg

    def _connect_ros_infra(self):
        rospy.loginfo("ƒêang ch·ªù service 'request_path_service'...")
        self.service_name = 'request_path_service'
        try:
            rospy.wait_for_service(self.service_name, timeout=30.0)
            self.RequestPath_client = rospy.ServiceProxy(self.service_name, RequestPath)
        except rospy.ROSException as e:
            raise RuntimeError(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi service 'request_path_service': {e}")

    # ===================================================================
    # H√ÄM RUN
    # ===================================================================
    def run(self):
        """H√†m ch·∫°y v√≤ng l·∫∑p hu·∫•n luy·ªán ch√≠nh."""
        self.sys_controller.cleanup_control_files()
        self.start_cycle = self.model_manager.load_checkpoints()

        rospy.loginfo("Ch·ªù message ƒë·∫ßu ti√™n t·ª´ /gazebo/model_states...")
        while self.latest_model_states is None and not rospy.is_shutdown():
            rospy.sleep(0.1)
        if rospy.is_shutdown(): return
        
        # --- Giai ƒëo·∫°n thi·∫øt l·∫≠p ban ƒë·∫ßu ---
        rospy.loginfo("Y√™u c·∫ßu c√°c qu·ªπ ƒë·∫°o ban ƒë·∫ßu...")
        all_paths_ok = True
        for name in self.robot_names:
            if not self.request_and_set_new_path(name):
                all_paths_ok = False
        
        if not all_paths_ok:
            rospy.logfatal("Kh√¥ng th·ªÉ l·∫•y ƒë∆∞·ª£c qu·ªπ ƒë·∫°o ban ƒë·∫ßu cho t·∫•t c·∫£ robot. D·ª´ng ch∆∞∆°ng tr√¨nh.")
            return # D·ª´ng l·∫°i n·∫øu kh√¥ng th·ªÉ b·∫Øt ƒë·∫ßu

        rospy.loginfo("ƒê√£ thi·∫øt l·∫≠p path ban ƒë·∫ßu. Reset m√¥i tr∆∞·ªùng...")
        for env in self.envs.values():
            env.reset()
        rospy.loginfo("Reset m√¥i tr∆∞·ªùng ho√†n t·∫•t.")

        # --- Giai ƒëo·∫°n hu·∫•n luy·ªán ch√≠nh ---
        train_cfg = self.cfg_manager.train_cfg
        rospy.loginfo(f"Chu·∫©n b·ªã v√†o v√≤ng l·∫∑p hu·∫•n luy·ªán. "
                      f"start_cycle = {self.start_cycle}, epochs = {train_cfg['epochs']}")
        
        last_cycle = self.start_cycle - 1 # Kh·ªüi t·∫°o gi√° tr·ªã an to√†n
        for cycle in range(self.start_cycle, train_cfg['epochs']):
            last_cycle = cycle
            rospy.loginfo(f"--- B·∫Øt ƒë·∫ßu Chu k·ª≥ {cycle + 1}/{train_cfg['epochs']} ---")
            self.reset_all_models()
            if self.sys_controller.check_for_signals() or rospy.is_shutdown():
                break

            self._collect_trajectories()
            
            if len(self.memory) > 0:
                self.trainer.update(self.memory)
            self.memory.clear()
            self._log_and_save(cycle)
        
        rospy.loginfo("ƒê√£ tho√°t kh·ªèi v√≤ng l·∫∑p hu·∫•n luy·ªán ch√≠nh.")
        self._shutdown(last_cycle)

    # ===================================================================
    # C√ÅC H√ÄM HELPER KH√ÅC
    # ===================================================================

    def _collect_trajectories(self):
        max_steps = self.cfg_manager.env_cfg['max_steps_per_episode']
        cycle_rewards = {name: 0.0 for name in self.robot_names}
        cycle_steps = {name: 0 for name in self.robot_names}
        
        for _ in range(max_steps):
            if self.latest_model_states is None or rospy.is_shutdown(): break
            
            current_model_states = self.latest_model_states
            observations_raw = [env.update_state_and_get_obs(current_model_states) for env in self.envs.values()]
            
            observations_np = np.array(observations_raw)
            for obs in observations_np:
                self.model_manager.obs_normalizer.observe(obs)
            observations_normalized = self.model_manager.obs_normalizer.normalize(observations_np)
            states_tensor = torch.FloatTensor(observations_normalized).to(self.model_manager.device)

            with torch.no_grad():
                actions, log_probs = self.model_manager.policy_net.get_action(states_tensor)
            
            actions_np = actions.cpu().numpy()
            log_probs_np = log_probs.cpu().numpy()
            
            for i, name in enumerate(self.robot_names):
                self.envs[name].step(actions_np[i],observations_raw[i][1])
            
            if self.envs: next(iter(self.envs.values())).rate.sleep()

            next_model_states = self.latest_model_states
            if next_model_states is None: break

            for i, name in enumerate(self.robot_names):
                env = self.envs[name]
                reward, done, goal_reached,crashed_status , min_obstacle_dist= env.calculate_reward_and_done(next_model_states,step=self.steps_in_task[name])
                
                # --- LOGIC X·ª¨ L√ù K·∫æT TH√öC EPISODE ƒê√öNG ƒê·∫ÆN ---
                timeout = self.steps_in_task[name] >= max_steps - 1 # -1 ƒë·ªÉ an to√†n
                if timeout:
                    reward += -100

                is_episode_end = done or timeout 
                
                self.memory.add(observations_normalized[i], actions_np[i], reward, is_episode_end, log_probs_np[i])
                
                self.steps_in_task[name] += 1
                cycle_rewards[name] += reward
                cycle_steps[name] += 1
                
                if is_episode_end:
                    if goal_reached:
                        rospy.loginfo(f"üéâ [{name}] ƒê√É T·ªöI ƒê√çCH! Reward cu·ªëi: {reward:.2f}, T·ªïng b∆∞·ªõc: {self.steps_in_task[name]}.")
                    elif crashed_status:
                        self.reset_robot_model(name) 
                        rospy.logwarn(f"üí• [{name}] Va ch·∫°m & Kh√¥ng tho√°t ƒë∆∞·ª£c! Reward cu·ªëi: {reward:.2f}, Dist: {min_obstacle_dist:.2f}, B·ªã k·∫πt trong {self.steps_in_task[name]} b∆∞·ªõc.")
                    # elif done and not goal_reached:
                    #     rospy.logwarn(f"üí• [{name}] Episode th·∫•t b·∫°i. Reward cu·ªëi: {reward:.2f}, T·ªïng b∆∞·ªõc: {self.steps_in_task[name]}.")
                    elif timeout:
                        rospy.loginfo(f"‚è∞ [{name}] H·∫øt th·ªùi gian. Reward cu·ªëi: {reward:.2f}, T·ªïng b∆∞·ªõc: {self.steps_in_task[name]}.")
                    
                    self.request_and_set_new_path(name)
                    env.reset_internal_state()
                    self.steps_in_task[name] = 0
        
        # --- C·∫¨P NH·∫¨T BI·∫æN LOGGING (ƒê√£ s·ª≠a) ---
        total_steps = sum(cycle_steps.values())
        self.last_cycle_avg_system_reward = sum(cycle_rewards.values()) / total_steps if total_steps > 0 else 0
        
        for name in self.robot_names:
            steps = cycle_steps[name]
            total_r = cycle_rewards[name]
            self.last_cycle_total_reward_per_robot[name] = total_r
            self.last_cycle_avg_reward_per_robot[name] = total_r / steps if steps > 0 else 0
    def request_and_set_new_path(self, robot_name, max_retries=3, retry_delay=1.0):
        """Y√™u c·∫ßu path m·ªõi, v·ªõi logic th·ª≠ l·∫°i n·∫øu th·∫•t b·∫°i."""
        for attempt in range(max_retries):
            try:
                env_to_set = self.envs[robot_name]
                point_list = [
                    Point(x=-17.7, y=14.6, z=0.0),
                    Point(x=-17.7, y=-16.5, z=0.0),
                    Point(x=-5.1, y=-16.6, z=0.0),
                    Point(x=10.2, y=-16.4, z=0.0),
                    Point(x=26.4, y=14.8, z=0.0),
                ]
                req = RequestPathRequest()
                req.robot_name = robot_name
                req.end_point = random.choice(point_list)


                response = self.RequestPath_client(req)
                
                if response.success and len(response.path.poses) > 1:
                    #rospy.loginfo(f"[{robot_name}] Nh·∫≠n ƒë∆∞·ª£c path th√†nh c√¥ng. v·ªõi {len(response.path.poses)} ƒëi·ªÉm.")
                    env_to_set.set_new_path(response.path)
                    return True
                else:
                    rospy.logwarn(f"[{robot_name}] Y√™u c·∫ßu path th·∫•t b·∫°i ho·∫∑c path r·ªóng: {response.message}. Th·ª≠ l·∫°i sau {retry_delay}s.")
                    rospy.sleep(retry_delay)

            except rospy.ServiceException as e:
                rospy.logerr(f"[{robot_name}] L·ªói g·ªçi service 'RequestPath': {e}. Th·ª≠ l·∫°i sau {retry_delay}s.")
                rospy.sleep(retry_delay)
            except KeyError:
                rospy.logerr(f"L·ªói nghi√™m tr·ªçng: Kh√¥ng t√¨m th·∫•y m√¥i tr∆∞·ªùng cho robot t√™n '{robot_name}'")
                return False

        rospy.logerr(f"[{robot_name}] Kh√¥ng th·ªÉ l·∫•y ƒë∆∞·ª£c path h·ª£p l·ªá sau {max_retries} l·∫ßn th·ª≠.")
        return False

    def _log_and_save(self, cycle):
        current_cycle = cycle + 1
        log_interval = self.cfg_manager.train_cfg.get('log_interval', 10)
        save_interval = self.cfg_manager.checkpoint_cfg.get('save_interval', 100)

        if current_cycle % log_interval == 0:
            rospy.loginfo("="*50)
            rospy.loginfo(f"K·∫øt qu·∫£ Chu k·ª≥ (Cycle) s·ªë {current_cycle}:")
            system_total_reward = sum(self.last_cycle_total_reward_per_robot.values())
            
            self.logger.log_cycle_data(
                cycle_num=current_cycle,
                total_rewards_per_robot=self.last_cycle_total_reward_per_robot,
                avg_rewards_per_robot=self.last_cycle_avg_reward_per_robot,
                system_total_reward=system_total_reward,
                system_avg_reward=self.last_cycle_avg_system_reward
            )
            for name in self.robot_names:
                total_reward = self.last_cycle_total_reward_per_robot.get(name, 0)
                avg_reward = self.last_cycle_avg_reward_per_robot.get(name, 0)
                rospy.loginfo(f"  - {name}: Total Reward = {total_reward:.2f}, Avg Reward/Step = {avg_reward:.3f}")
            
            total_system_reward = sum(self.last_cycle_total_reward_per_robot.values())
            rospy.loginfo(f"--> Total System Reward: {total_system_reward:.2f}")
            rospy.loginfo(f"--> Avg System Reward/Step: {self.last_cycle_avg_system_reward:.3f}")
            rospy.loginfo("="*50)

        if current_cycle % save_interval == 0:
            self.model_manager.save_checkpoint(current_cycle)
            

    def _shutdown(self, last_cycle):
        rospy.loginfo("ƒêang d·ªçn d·∫πp v√† k·∫øt th√∫c...")
        self.model_manager.save_final_models(last_cycle + 1)
        for env in self.envs.values():
            env.close()
        rospy.loginfo("Ho√†n t·∫•t.")
    def reset_all_models(self):

        for i, name in enumerate(self.robot_names):
            state = ModelState()
            state.model_name = name
            
            # M·ªói robot l·ªách ra theo tr·ª•c x, tr√°nh tr√πng nhau
            base_x = 21.5
            offset = 2.0  # kho·∫£ng c√°ch gi·ªØa c√°c robot
            state.pose.position.x = base_x + i * offset
            state.pose.position.y = 15.0
            state.pose.position.z = 0.0

            # H∆∞·ªõng robot theo tr·ª•c x
            quat = quaternion_from_euler(0, 0, 0.0)
            state.pose.orientation.x = quat[0]
            state.pose.orientation.y = quat[1]
            state.pose.orientation.z = quat[2]
            state.pose.orientation.w = quat[3]

            # Kh√¥ng c√≥ v·∫≠n t·ªëc ban ƒë·∫ßu
            state.twist.linear.x = 0.0
            state.twist.angular.z = 0.0
            state.reference_frame = "world"

            try:
                resp = self.set_model_state(state)
                if resp.success:
                    rospy.loginfo(f"[{name}] ‚úÖ Reset model th√†nh c√¥ng.")
                else:
                    rospy.logwarn(f"[{name}] ‚ùå Reset model th·∫•t b·∫°i: {resp.status_message}")
            except rospy.ServiceException as e:
                rospy.logerr(f"[{name}] üö® L·ªói khi g·ªçi reset model: {e}")
    def reset_robot_model(self, robot_name_to_reset):
        """
        Reset v·ªã tr√≠ v√† tr·∫°ng th√°i c·ªßa m·ªôt robot c·ª• th·ªÉ trong Gazebo.

        Args:
            robot_name_to_reset (str): T√™n c·ªßa model robot c·∫ßn reset.
        """
        
        # Ki·ªÉm tra xem robot c√≥ t·ªìn t·∫°i trong danh s√°ch qu·∫£n l√Ω kh√¥ng
        if robot_name_to_reset not in self.robot_names:
            rospy.logerr(f"üö® T√™n robot '{robot_name_to_reset}' kh√¥ng c√≥ trong danh s√°ch qu·∫£n l√Ω. Kh√¥ng th·ªÉ reset.")
            return False

        state = ModelState()
        state.model_name = robot_name_to_reset

        # --- Logic x√°c ƒë·ªãnh v·ªã tr√≠ reset ---
        # ·ªû ƒë√¢y, ch√∫ng ta v·∫´n d√πng logic c≈© d·ª±a tr√™n ch·ªâ s·ªë (index) c·ªßa robot
        # ƒë·ªÉ ƒë·∫£m b·∫£o m·ªói robot lu√¥n ƒë∆∞·ª£c reset v·ªÅ ƒë√∫ng v·ªã tr√≠ ƒë∆∞·ª£c g√°n cho n√≥,
        # tr√°nh vi·ªác reset ch·ªìng l√™n nhau.
        try:
            robot_index = self.robot_names.index(robot_name_to_reset)
        except ValueError:
            # Tr∆∞·ªùng h·ª£p n√†y ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra ·ªü tr√™n, nh∆∞ng ƒë·ªÉ cho ch·∫Øc ch·∫Øn
            rospy.logerr(f"L·ªói kh√¥ng mong mu·ªën: kh√¥ng t√¨m th·∫•y index cho '{robot_name_to_reset}'.")
            return False

        base_x = 21.5
        offset = 2.0  # kho·∫£ng c√°ch gi·ªØa c√°c v·ªã tr√≠ reset c·ªßa robot
        state.pose.position.x = base_x + robot_index * offset
        state.pose.position.y = 15.0
        state.pose.position.z = 0.0

        # H∆∞·ªõng robot v·ªÅ ph√≠a tr∆∞·ªõc (d·ªçc theo tr·ª•c x)
        quat = quaternion_from_euler(0, 0, 0.0)
        state.pose.orientation.x = quat[0]
        state.pose.orientation.y = quat[1]
        state.pose.orientation.z = quat[2]
        state.pose.orientation.w = quat[3]

        # ƒê·∫∑t v·∫≠n t·ªëc v·ªÅ 0
        state.twist.linear.x = 0.0
        state.twist.linear.y = 0.0
        state.twist.linear.z = 0.0
        state.twist.angular.x = 0.0
        state.twist.angular.y = 0.0
        state.twist.angular.z = 0.0
        
        state.reference_frame = "world"

        try:
            # S·ª≠ d·ª•ng service proxy ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o m·ªôt l·∫ßn
            resp = self.set_model_state(state)
            if resp.success:
                rospy.loginfo(f"[{robot_name_to_reset}] ‚úÖ Reset model th√†nh c√¥ng v·ªÅ v·ªã tr√≠ ban ƒë·∫ßu.")
                return True
            else:
                rospy.logwarn(f"[{robot_name_to_reset}] ‚ùå Reset model th·∫•t b·∫°i: {resp.status_message}")
                return False
        except rospy.ServiceException as e:
            rospy.logerr(f"[{robot_name_to_reset}] üö® L·ªói khi g·ªçi service set_model_state: {e}")
            return False